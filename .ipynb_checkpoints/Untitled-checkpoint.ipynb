{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "import gensim\n",
    "import re\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def read_data(file_name,is_train=True):\n",
    "    data_df = pd.read_csv(file_name,encoding='gbk')\n",
    "    if is_train:\n",
    "        data_df.columns = ['names','price','catgory']\n",
    "        data_df['catgory'] = data_df.catgory - 100\n",
    "    else:\n",
    "        data_df.columns = ['names','price']\n",
    "    data_df['length'] = data_df.names.apply(lambda x:len(x))\n",
    "    return data_df\n",
    "\n",
    "# def cut_seq(x):\n",
    "\n",
    "train_df = read_data('./train.csv')\n",
    "test_df = read_data('./test.csv',is_train=False)\n",
    "\n",
    "stop_word_df = pd.read_csv('./stopwords.txt',sep='\\t',quoting=3,index_col=False,names=['word'])\n",
    "stop_word_df = stop_word_df['word'].values.tolist()\n",
    "\n",
    "train_df['cutted'] = train_df.names.apply( lambda x: jieba.lcut(x) )\n",
    "train_df['cutted'] = train_df.cutted.apply(lambda x: \" \".join([w for w in x if x not in stop_word_df]))\n",
    "\n",
    "test_df['cutted'] = test_df.names.apply( lambda x: jieba.lcut(x) )\n",
    "test_df['cutted'] = test_df.cutted.apply(lambda x: \" \".join([w for w in x if x not in stop_word_df]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>names</th>\n",
       "      <th>price</th>\n",
       "      <th>catgory</th>\n",
       "      <th>length</th>\n",
       "      <th>cutted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5224</th>\n",
       "      <td>柠檬宝宝LEMONKID冬季室内加厚防滑卡通包跟软底宝宝居家鞋男女童棉拖鞋LE061016粉...</td>\n",
       "      <td>77</td>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>柠檬 宝宝 LEMONKID 冬季 室内 加厚 防滑 卡通 包 跟 软底 宝宝 居家 鞋 男...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8198</th>\n",
       "      <td>小蚁智能摄像机夜视版升级1080PWIFI网络摄像头监控摄像头智能家居支持小米路由WIFI本地存储</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>小蚁 智能 摄像机 夜视 版 升级 1080PWIFI 网络 摄像头 监控 摄像头 智能家居...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>阿波罗之梦AD162玫瑰花爱心杯子套装送女友老婆女生生日礼物实用闺蜜妈妈情人节创意礼物表白结婚纪</td>\n",
       "      <td>76</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>阿波罗 之梦 AD162 玫瑰花 爱心 杯子 套装 送 女友 老婆 女生 生日礼物 实用 闺...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7094</th>\n",
       "      <td>速比涛SPEEDO新品全新设计TPR柔软游泳耳塞防水舒适防水导音游泳耳塞游泳装备配件夜深黑</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "      <td>速比 涛 SPEEDO 新品 全新 设计 TPR 柔软 游泳 耳塞 防水 舒适 防水 导音 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5408</th>\n",
       "      <td>欧时纳JUSTSTAR女包新款女士包包手提包韩版单肩斜挎包休闲百搭潮流小方包JS149</td>\n",
       "      <td>76</td>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "      <td>欧时纳 JUSTSTAR 女包 新款 女士 包包 手提包 韩版 单肩 斜挎包 休闲 百搭 潮...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  names  price  catgory  \\\n",
       "5224  柠檬宝宝LEMONKID冬季室内加厚防滑卡通包跟软底宝宝居家鞋男女童棉拖鞋LE061016粉...     77        4   \n",
       "8198  小蚁智能摄像机夜视版升级1080PWIFI网络摄像头监控摄像头智能家居支持小米路由WIFI本地存储     76        9   \n",
       "2      阿波罗之梦AD162玫瑰花爱心杯子套装送女友老婆女生生日礼物实用闺蜜妈妈情人节创意礼物表白结婚纪     76        6   \n",
       "7094      速比涛SPEEDO新品全新设计TPR柔软游泳耳塞防水舒适防水导音游泳耳塞游泳装备配件夜深黑     76        9   \n",
       "5408        欧时纳JUSTSTAR女包新款女士包包手提包韩版单肩斜挎包休闲百搭潮流小方包JS149     76        4   \n",
       "\n",
       "      length                                             cutted  \n",
       "5224      50  柠檬 宝宝 LEMONKID 冬季 室内 加厚 防滑 卡通 包 跟 软底 宝宝 居家 鞋 男...  \n",
       "8198      49  小蚁 智能 摄像机 夜视 版 升级 1080PWIFI 网络 摄像头 监控 摄像头 智能家居...  \n",
       "2         48  阿波罗 之梦 AD162 玫瑰花 爱心 杯子 套装 送 女友 老婆 女生 生日礼物 实用 闺...  \n",
       "7094      45  速比 涛 SPEEDO 新品 全新 设计 TPR 柔软 游泳 耳塞 防水 舒适 防水 导音 ...  \n",
       "5408      43  欧时纳 JUSTSTAR 女包 新款 女士 包包 手提包 韩版 单肩 斜挎包 休闲 百搭 潮...  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sort_values(by='length',ascending=False).head()\n",
    "# test_df.sort_values(by='price',ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把数据提取为list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 测试集中的文字及y标签\n",
    "X = [i for i in train_df.cutted]\n",
    "Y = [i for i in train_df.catgory]\n",
    "# 测试集中的文字\n",
    "test_text = [i for i in train_df.cutted]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=4000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(\n",
    "    analyzer='word',\n",
    "    max_features=4000\n",
    ")\n",
    "vec.fit(X+test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.696\n",
      "[[3.14865395e-03 2.56072336e-04 3.19937770e-02 2.91143675e-02\n",
      "  9.45331389e-03 6.19073706e-01 3.71596771e-02 9.11693847e-02\n",
      "  1.78117879e-01 5.13167939e-04]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sklearn\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(vec.transform(X_train),y_train)\n",
    "print(classifier.score(vec.transform(X_test),y_test))\n",
    "print(classifier.predict_proba(vec.transform([X_test[0]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method predict in module sklearn.naive_bayes:\n",
      "\n",
      "predict(X) method of sklearn.naive_bayes.MultinomialNB instance\n",
      "    Perform classification on an array of test vectors X.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : array-like, shape = [n_samples, n_features]\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    C : array, shape = [n_samples]\n",
      "        Predicted target values for X\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(classifier.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.655\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "classifier = SVC(kernel='linear')\n",
    "classifier.fit(vec.transform(X_train),y_train)\n",
    "print(classifier.score(vec.transform(X_test),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 该函数可以输出属于每个类的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3380)\t1\n",
      "  (0, 3475)\t1\n"
     ]
    }
   ],
   "source": [
    "print(vec.transform([X_test[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "基于卷积神经网络的中文文本分类\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "learn = tf.contrib.learn\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "#文档最长长度\n",
    "MAX_DOCUMENT_LENGTH = 100\n",
    "#最小词频数\n",
    "MIN_WORD_FREQUENCE = 2\n",
    "#词嵌入的维度\n",
    "EMBEDDING_SIZE = 20\n",
    "#filter个数\n",
    "N_FILTERS = 10\n",
    "#感知野大小\n",
    "WINDOW_SIZE = 20\n",
    "#filter的形状\n",
    "FILTER_SHAPE1 = [WINDOW_SIZE, EMBEDDING_SIZE]\n",
    "FILTER_SHAPE2 = [WINDOW_SIZE, N_FILTERS]\n",
    "#池化\n",
    "POOLING_WINDOW = 4\n",
    "POOLING_STRIDE = 2\n",
    "n_words = 0\n",
    "\n",
    "\n",
    "def cnn_model(features, target):\n",
    "\t\"\"\"\n",
    "    2层的卷积神经网络，用于短文本分类\n",
    "    \"\"\"\n",
    "\t# 先把词转成词嵌入\n",
    "\t# 我们得到一个形状为[n_words, EMBEDDING_SIZE]的词表映射矩阵\n",
    "\t# 接着我们可以把一批文本映射成[batch_size, sequence_length, EMBEDDING_SIZE]的矩阵形式\n",
    "\ttarget = tf.one_hot(target, 15, 1, 0)\n",
    "\tword_vectors = tf.contrib.layers.embed_sequence(\n",
    "\t\t\tfeatures, vocab_size=n_words, embed_dim=EMBEDDING_SIZE, scope='words')\n",
    "\tword_vectors = tf.expand_dims(word_vectors, 1)\n",
    "\twith tf.variable_scope('CNN_Layer1'):\n",
    "\t\t# 添加卷积层做滤波\n",
    "\t\tconv1 = tf.contrib.layers.convolution2d(\n",
    "\t\t\t\tword_vectors, N_FILTERS, FILTER_SHAPE1, padding='VALID')\n",
    "\t\t# 添加RELU非线性\n",
    "\t\tconv1 = tf.nn.relu(conv1)\n",
    "\t\t# 最大池化\n",
    "\t\tpool1 = tf.nn.max_pool(\n",
    "\t\t\t\tconv1,\n",
    "\t\t\t\tksize=[1, POOLING_WINDOW, 1, 1],\n",
    "\t\t\t\tstrides=[1, POOLING_STRIDE, 1, 1],\n",
    "\t\t\t\tpadding='SAME')\n",
    "\t\t# 对矩阵进行转置，以满足形状\n",
    "\t\tpool1 = tf.transpose(pool1, [0, 1, 3, 2])\n",
    "\twith tf.variable_scope('CNN_Layer2'):\n",
    "\t\t# 第2个卷积层\n",
    "\t\tconv2 = tf.contrib.layers.convolution2d(\n",
    "                pool1, N_FILTERS, FILTER_SHAPE2, padding='VALID')\n",
    "\t\t# 抽取特征\n",
    "\t\tpool2 = tf.squeeze(tf.reduce_max(conv2, 1), squeeze_dims=[1])\n",
    "\n",
    "\t# 全连接层\n",
    "\tlogits = tf.contrib.layers.fully_connected(pool2, 15, activation_fn=None)\n",
    "\tloss = tf.losses.softmax_cross_entropy(target, logits)\n",
    "\n",
    "\ttrain_op = tf.contrib.layers.optimize_loss(\n",
    "\t\t\tloss,\n",
    "\t\t\ttf.train.get_global_step,\n",
    "\t\t\toptimizer='Adam',\n",
    "\t\t\tlearning_rate=0.01)\n",
    "\n",
    "\treturn ({\n",
    "\t\t\t'class': tf.argmax(logits, 1),\n",
    "\t\t\t'prob': tf.nn.softmax(logits)\n",
    "\t}, loss, train_op)\n",
    "\n",
    "\n",
    "#构建数据集\n",
    "#x_train = pandas.DataFrame(train_data)[1]\n",
    "#y_train = pandas.Series(train_target)\n",
    "#x_test = pandas.DataFrame(test_data)[1]\n",
    "#y_test = pandas.Series(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1, 2, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([1, 2, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = ['I am good', 'you are here', 'I am glad', 'it is great']\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(10, min_frequency=1)\n",
    "list(vocab_processor.fit_transform(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 2762\n"
     ]
    }
   ],
   "source": [
    "global n_words\n",
    "# 处理词汇\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH, min_frequency=MIN_WORD_FREQUENCE)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(X_train)))\n",
    "x_test = np.array(list(vocab_processor.transform(X_test)))\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_dic = {'technology':1, 'car':2, 'entertainment':3, 'military':4, 'sports':5}\n",
    "# train_target = map(lambda x:cate_dic[x], train_target)\n",
    "# test_target = map(lambda x:cate_dic[x], test_target)\n",
    "train_target = y_train\n",
    "test_target = y_test\n",
    "\n",
    "y_train = pandas.Series(train_target)\n",
    "y_test = pandas.Series(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/f2/djk1m_h90_b43vq4lr785cb80000gn/T/tmpvporszbm\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1c2dd1a3c8>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': '/var/folders/f2/djk1m_h90_b43vq4lr785cb80000gn/T/tmpvporszbm'}\n",
      "WARNING:tensorflow:From /Volumes/05/anconda/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:731: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-97-3b89f14e18d1>:74: get_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_global_step\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/f2/djk1m_h90_b43vq4lr785cb80000gn/T/tmpvporszbm/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 14.7077\n",
      "INFO:tensorflow:loss = 0.0, step = 101 (6.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 15.0225\n",
      "INFO:tensorflow:loss = 0.0, step = 201 (6.657 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.8078\n",
      "INFO:tensorflow:loss = 0.0, step = 301 (6.753 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.2353\n",
      "INFO:tensorflow:loss = 0.0, step = 401 (7.025 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.2401\n",
      "INFO:tensorflow:loss = 0.0, step = 501 (7.023 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.481\n",
      "INFO:tensorflow:loss = 0.0, step = 601 (7.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.149\n",
      "INFO:tensorflow:loss = 0.0, step = 701 (7.605 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.2689\n",
      "INFO:tensorflow:loss = 0.0, step = 801 (7.010 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.9833\n",
      "INFO:tensorflow:loss = 0.0, step = 901 (6.673 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/f2/djk1m_h90_b43vq4lr785cb80000gn/T/tmpvporszbm/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/f2/djk1m_h90_b43vq4lr785cb80000gn/T/tmpvporszbm/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "classifier = learn.SKCompat(learn.Estimator(model_fn=cnn_model))\n",
    "\n",
    "# 训练和预测\n",
    "classifier.fit(x_train, y_train, steps=1000)\n",
    "y_predicted = classifier.predict(x_test)['class']\n",
    "score = metrics.accuracy_score(y_test, y_predicted)\n",
    "print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2063,    0,   74, ...,    0,    0,    0],\n",
       "       [1764, 2671, 2482, ...,    0,    0,    0],\n",
       "       [   0,    0, 1266, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   0,    0,  177, ...,    0,    0,    0],\n",
       "       [2203, 1133,  418, ...,    0,    0,    0],\n",
       "       [2669,  519,    0, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
